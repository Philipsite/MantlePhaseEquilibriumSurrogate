
using Flux
using CSV, DataFrames
using JLD2
using Sprout
using CairoMakie

n_layers = [2, 3, 4, 5, 6, 7, 8, 9];
n_neurons = [50, 100, 150, 200, 250, 300, 350, 400];
fraction_backbone_layers = [1//2, 2//3];
batch_size = [4096, 25000, 100000];

masking_f = (clas_out, reg_out) -> (mask_ğ‘£(clas_out, reg_out[1]), mask_ğ—(clas_out, reg_out[2]));

# LOAD DATA
#-----------------------------------------------------------------------
DATA_DIR = joinpath("data", "generated_dataset")
x_train = CSV.read(joinpath(DATA_DIR, "sb21_02Oct25_train_x.csv"), DataFrame);
y_train = CSV.read(joinpath(DATA_DIR, "sb21_02Oct25_train_y.csv"), DataFrame);
x_val = CSV.read(joinpath(DATA_DIR, "sb21_02Oct25_val_x.csv"), DataFrame);
y_val = CSV.read(joinpath(DATA_DIR, "sb21_02Oct25_val_y.csv"), DataFrame);

x_train, ğ‘£_train, ğ—_ss_train, Ï_train, Îš_train, Î¼_train = preprocess_data(x_train, y_train);
x_val, ğ‘£_val, ğ—_ss_val, Ï_val, Îš_val, Î¼_val = preprocess_data(x_val, y_val);

# Normalise inputs
xNorm = Norm(x_train);
x_train = xNorm(x_train);
x_val = xNorm(x_val);

# Scale outputs
ğ—Scale = MinMaxScaler(ğ—_ss_train);
ğ—_ss_train = ğ—Scale(ğ—_ss_train);
ğ—_ss_val = ğ—Scale(ğ—_ss_val);

ğ‘£Scale = MinMaxScaler(ğ‘£_train);
ğ‘£_train = ğ‘£Scale(ğ‘£_train);
ğ‘£_val = ğ‘£Scale(ğ‘£_val);

# SETUP LOSS & METRICS
#----------------------------------------------------------------------
# Normalisation/scaling structures must live on the same device as the model is trained on
# for training on GPU move normalisers/scalers/pure_phase_comp to GPU; e.g. xNorm_gpu = xNorm |> gpu
xNorm_gpu = xNorm |> gpu;
ğ‘£Scale_gpu = ğ‘£Scale |> gpu;
ğ—Scale_gpu = ğ—Scale |> gpu;
pp_mat_gpu = reshape(PP_COMP_adj, 6, :) |> gpu;

function loss((ğ‘£_Å·, ğ—_Å·), (ğ‘£, ğ—), x)
    return sum(abs2, ğ‘£_Å· .- ğ‘£) + sum(abs2, ğ—_Å· .- ğ—) + misfit.mass_balance_abs_misfit((descale(ğ‘£Scale_gpu, ğ‘£_Å·), descale(ğ—Scale_gpu, ğ—_Å·)), denorm(xNorm_gpu, x)[3:end,:,:], agg=sum, pure_phase_comp=pp_mat_gpu) + misfit.closure_condition((descale(ğ‘£Scale_gpu, ğ‘£_Å·), descale(ğ—Scale_gpu, ğ—_Å·)), (ğ‘£, ğ—), agg=sum)
end
# Metrics (for validation only, must follow signature (Å·, y) -> Real)
function mass_balance_metric((ğ‘£_Å·, ğ—_Å·), (_, _))
    return misfit.mass_balance_abs_misfit((descale(ğ‘£Scale, ğ‘£_Å·), descale(ğ—Scale, ğ—_Å·)), denorm(xNorm, x_val)[3:end,:,:], agg=mean)
end
function mae_ğ‘£(Å·, y)
    return misfit.mae_no_zeros(descale(ğ‘£Scale, Å·[1]), descale(ğ‘£Scale, y[1]))
end
function mae_ğ—(Å·, y)
    return misfit.mae_no_zeros(descale(ğ—Scale, Å·[2]), descale(ğ—Scale, y[2]))
end
function closure_metric((ğ‘£_Å·, ğ—_Å·), y)
    return misfit.closure_condition((descale(ğ‘£Scale, ğ‘£_Å·), descale(ğ—Scale, ğ—_Å·)), y, agg=mean)
end

metrics = [mass_balance_metric, mae_ğ‘£, mae_ğ—, closure_metric];

# TUNE IT
#-----------------------------------------------------------------------
hpt_regressor_common_backbone(n_layers, n_neurons, fraction_backbone_layers[1], batch_size[1], loss,
                              (x_train, (ğ‘£_train, ğ—_ss_train)), (x_val, (ğ‘£_val, ğ—_ss_val)),
                              masking_f,
                              1000, metrics,
                              lr_schedule = false,
                              subdir_appendix = "cBack_fbl1_bs1")

hpt_regressor_common_backbone(n_layers, n_neurons, fraction_backbone_layers[1], batch_size[2], loss,
                              (x_train, (ğ‘£_train, ğ—_ss_train)), (x_val, (ğ‘£_val, ğ—_ss_val)),
                              masking_f,
                              1000, metrics,
                              lr_schedule = false,
                              subdir_appendix = "cBack_fbl1_bs2")

hpt_regressor_common_backbone(n_layers, n_neurons, fraction_backbone_layers[1], batch_size[3], loss,
                              (x_train, (ğ‘£_train, ğ—_ss_train)), (x_val, (ğ‘£_val, ğ—_ss_val)),
                              masking_f,
                              1000, metrics,
                              lr_schedule = false,
                              subdir_appendix = "cBack_fbl1_bs3")

hpt_regressor_common_backbone(n_layers, n_neurons, fraction_backbone_layers[2], batch_size[1], loss,
                              (x_train, (ğ‘£_train, ğ—_ss_train)), (x_val, (ğ‘£_val, ğ—_ss_val)),
                              masking_f,
                              1000, metrics,
                              lr_schedule = false,
                              subdir_appendix = "cBack_fbl2_bs1")

hpt_regressor_common_backbone(n_layers, n_neurons, fraction_backbone_layers[2], batch_size[2], loss,
                              (x_train, (ğ‘£_train, ğ—_ss_train)), (x_val, (ğ‘£_val, ğ—_ss_val)),
                              masking_f,
                              1000, metrics,
                              lr_schedule = false,
                              subdir_appendix = "cBack_fbl2_bs2")

hpt_regressor_common_backbone(n_layers, n_neurons, fraction_backbone_layers[2], batch_size[3], loss,
                              (x_train, (ğ‘£_train, ğ—_ss_train)), (x_val, (ğ‘£_val, ğ—_ss_val)),
                              masking_f,
                              1000, metrics,
                              lr_schedule = false,
                              subdir_appendix = "cBack_fbl2_bs3")

println("Hyperparameter tuning complete!")
